{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# IMDB Sentiment Analyses\n\nNeste notebook estamos utilizando os dados do Kaggle (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n\nVamos seguir os seguintes passos:\n\n1. Importar o Dataset\n2. Analisar os Dados\n3. Preparar os Dados para Construir o Modelo\n4. Criar o Dataset de Teste e Treino\n5. Treino do Modelo Utilizando Diferentes Algoritmos\n6. Avalia\u00e7\u00e3o dos Modelos\n7. Sele\u00e7\u00e3o do Melhor Modelo\n8. Deploy do Modelo para o Watson Machine Learning\n9. Avalia\u00e7\u00e3o do Modelo Final"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install nltk"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "%matplotlib inline \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn import ensemble\nfrom sklearn import neighbors\nfrom sklearn import linear_model\nfrom sklearn import metrics"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import nltk\nnltk.download('stopwords')\nnltk.download('punkt')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 1 - Importar o Dataset"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 2 - Analisar dos dados"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df = df_data_1\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.describe()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 3 - Preparar os Dados para Construir o Modelo\n\nAgora vamos preparar nossos seguindo os passos:\n\n    1. Tonkenization         \n    2. Remover stopwords\n    3. Stemming text\n    4. Juntar novamente em uma \u00fanica frase\n    \nComo estamos trabalhando com uma entrada de texto, realizamos estas etapas para \"normalizar\" nossa base."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "stop_words = stopwords.words('english')\nporter_stemmer = PorterStemmer()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def identify_tokens(row):\n    source = row[0]\n    tokens = word_tokenize(source)\n    token_words = [w for w in tokens if w.isalpha()]\n    return token_words"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def remove_stops(row):\n    source_tokenization = row[2]\n    stop = [w for w in source_tokenization if not w in stop_words]\n    return (stop)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def stem_porter(row):\n    my_list = row[2]\n    stemmed_list = [porter_stemmer.stem(word) for word in my_list]\n    return (stemmed_list)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def rejoin_words(row):\n    my_list = row[2]\n    joined_words = (\" \".join(my_list))\n    return joined_words"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def pre_processing(df):\n    print('Tokenization')\n    df['text1'] = df.apply(identify_tokens, axis=1)\n    print('Remove stop words')\n    df['text1'] = df.apply(remove_stops, axis=1)\n    print('Stemming')\n    df['text1'] = df.apply(stem_porter, axis=1)\n    print('Rejoin words')\n    df['tidy_text'] = df.apply(rejoin_words, axis=1)\n    \n    return df"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df = pre_processing(df)\n\ndf['tidy_text'] = df['tidy_text'].str.lower()\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 4 - Criar o Dataset de Teste e Treino\n\nVamos criar o nosso dataset de teste (30%) e treino (70%) de forma balanceado (Stratified)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X = df['tidy_text']\nY = df['sentiment']\n\nprint(X.shape)\nprint(Y.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Os modelos de Machine Learning ou Deep Learning esperam como entrada \"X\" um valor num\u00e9rico. Como estamos trabalhando com texto iremos realizar o processo de TfIdf para transformar o texto em valores num\u00e9ricos."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(2,3), sublinear_tf=True)\n\nX_train_tf = tfidf.fit_transform(X_train)\nX_test_tf = tfidf.transform(X_test)\n\nprint(Y.value_counts().shape)\nprint(X_train_tf.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "le = preprocessing.LabelEncoder()\n\nY_train_le = le.fit_transform(list(Y_train))\nY_test_le = le.transform(list(Y_test))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 5 - Treino do Modelo Utilizando Diferentes Algoritmos"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# binary classifiers\n# GradientBoostingClassifier\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X_train_tf, Y_train_le)\nY_predict_gradient_boost = gradient_boost.predict(X_test_tf)\nprint('Gradient Boosting Classifier DONE!')\n\n# SVC\nsvc_model = SVC(gamma='auto', kernel='sigmoid', C=1.8, probability=True)\nsvc_model.fit(X_train_tf, Y_train_le)\nY_predict_svm = svc_model.predict(X_test_tf)\nprint('Support Vector Machine(SVM) DONE!')\n\n# RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(X_train_tf, Y_train_le)\nY_predict_random_forest = random_forest.predict(X_test_tf)\nprint('Random Forest Classifier DONE!')\n\n# KNeighborsClassifier\nk_neighbors = KNeighborsClassifier()\nk_neighbors.fit(X_train_tf, Y_train_le)\nY_predict_k_neighbors = k_neighbors.predict(X_test_tf)\nprint('K Nearest Neighbor Classifier DONE!')\n\n# LogisticRegression\nlogistic_regression = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\nlogistic_regression.fit(X_train_tf, Y_train_le)\nY_predict_logistic_regression = logistic_regression.predict(X_test_tf)\nprint('Logistic Regression DONE!')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Gradient Boosting Classifier:  ', metrics.accuracy_score(Y_test_le, Y_predict_gradient_boost))\nprint('Support Vector Machine(SVM):   ', metrics.accuracy_score(Y_test_le, Y_predict_svm))\nprint('Random Forest Classifier:      ', metrics.accuracy_score(Y_test_le, Y_predict_random_forest))\nprint('K Nearest Neighbor Classifier: ', metrics.accuracy_score(Y_test_le, Y_predict_k_neighbors))\nprint('Logistic Regression:           ', metrics.accuracy_score(Y_test_le, Y_predict_logistic_regression))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 6 - Avalia\u00e7\u00e3o dos Modelos"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 6.1 - Support Vector Machines"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "svm_svc_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_svm)\nsns.heatmap(svm_svc_conf_matrix, annot=True,  fmt='');\ntitle = 'SVM'\nplt.title(title);"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 6.2 - Random Forest"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "random_forest_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_random_forest)\nsns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\ntitle = 'Random Forest'\nplt.title(title);"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 6.3 - Logistic Regression"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "logistic_regression_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_logistic_regression)\nsns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\ntitle = 'Logistic Regression'\nplt.title(title);"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 6.4 - Resumo de classifica\u00e7\u00e3o"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_svm)))\nprint('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_random_forest)))\nprint('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_logistic_regression)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 7 - Sele\u00e7\u00e3o do Melhor Modelo"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X_train_final = tfidf.fit_transform(X)\nY_train_final = le.fit_transform(list(Y))\n\nprint(X_train_final.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lrc = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\nlrc.fit(X_train_final, Y_train_final)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 8 - Deploy do Modelo para o Watson Machine Learning"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n\nPara nos autenticar no Watson Machine Learning no IBM Cloud, voc\u00ea precisa da api_key e location do seu servi\u00e7o.\n\nPodemos utilizar o [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) ou diretamente pelo portal do IBM Cloud.\n\nUsando o IBM Cloud CLI:\n\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n\nNOTE: Voc\u00ea pode obter a URL do servi\u00e7o indo at\u00e9 [Endpoint URLs section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning)."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "api_key = 'YOUR_API_KEY'\nlocation = 'YOUR_LOCATION'"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": location\n}"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 8.1 - Instalando a biblioteca do Watson Machine Learning\n\nNOTE: Documenta\u00e7\u00e3o pode ser encontrada [aqui](http://ibm-wml-api-pyclient.mybluemix.net/)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install -U ibm-watson-machine-learning"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watson_machine_learning import APIClient\n\nclient = APIClient(wml_credentials)\nprint(client.version)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 8.2 - Criando nosso espa\u00e7o de implementa\u00e7\u00e3o\n\nPrimeiro, crie um espa\u00e7o de implementa\u00e7\u00e3o que ser\u00e1 usado para fazer o deploy do nosso modelo. Caso ainda n\u00e3o tenha criado siga os passos abaixo.\n\n    Clique em Novo Espa\u00e7o de Implementa\u00e7\u00e3o\n    Crie um novo espa\u00e7o vazio\n    Selecione Cloud Object Storage\n    Selecione Watson Machine Learning e clique em Criar\n    Copie space_id e cole abaixo"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "space_id = 'YOUR_SPACE_ID'"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "client.spaces.list(limit=10)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "client.set.default_space(space_id)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "sofware_spec_uid = client.software_specifications.get_id_by_name(\"default_py3.7\")\nmetadata = {\n            client.repository.ModelMetaNames.NAME: 'Logistic Regression model to predict IMDB reviews',\n            client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n}\n\npublished_model = client.repository.store_model(\n    model=lrc,\n    meta_props=metadata)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "published_model_uid = client.repository.get_model_uid(published_model)\nmodel_details = client.repository.get_details(published_model_uid)\nprint(json.dumps(model_details, indent=2))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "client.repository.list_models()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# client.repository.delete('GUID of stored model')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of IMDB reviews\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\ncreated_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Get deployment UID and show details on the deployment\ndeployment_uid = client.deployments.get_uid(created_deployment)\nclient.deployments.get_details(deployment_uid)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "client.deployments.list()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#client.deployments.delete('GUID of deployed model')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 9 - Avalia\u00e7\u00e3o do Modelo Final\n\nAgora vamos enviar dados para o web service usando o m\u00e9todo score do WML."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# get scoring end point\nscoring_endpoint = client.deployments.get_scoring_href(created_deployment)\nprint(scoring_endpoint)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# add some test data\nscoring_payload = {\"input_data\": [\n    {'values': X_test_tf.toarray()\n    }]}"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# score the model\npredictions = client.deployments.score(deployment_uid, scoring_payload)\nprint('prediction',json.dumps(predictions, indent=2))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Y_predict_final_model = []\nfor y in predictions['predictions'][0]['values']:\n    Y_predict_final_model.append(y[0])\n    \nprint('Final Model WML:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_final_model)))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}