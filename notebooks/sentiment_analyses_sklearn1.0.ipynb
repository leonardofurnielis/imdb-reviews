{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# watsonx.ai: Deploy IMDB reviews sentiment analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Kaggle dataset (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Set up the environment](#setup_environment)\n",
    "1. [Explore and prepare training data](#explore_prepare_data)\n",
    "1. [Create train and test dataset](#train_test_set)\n",
    "1. [Train the model](#train_model)\n",
    "1. [Save the model](#save_model)\n",
    "1. [Deploy and score](#deploy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# This try-except block addresses SSL certificate verification issues.\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup_environment\"></a>\n",
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To authenticate to Watson Machine Learning in the IBM Cloud, you need api_key and service location.\n",
    "\n",
    "Using [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) or directly through the IBM Cloud portal.\n",
    "\n",
    "Using IBM Cloud CLI:\n",
    "\n",
    "```\n",
    "ibmcloud login\n",
    "ibmcloud iam api-key-create API_KEY_NAME\n",
    "```\n",
    "\n",
    "NOTE: To get the Service URL [Endpoint URLs section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning).\n",
    "\n",
    "**Action**: Enter your api_key and location in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'API_KEY'\n",
    "LOCATION = 'LOCATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = {\n",
    "    \"apikey\": API_KEY,\n",
    "    \"url\": LOCATION\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Assign space ID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_ID = 'SPACE_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Assign project ID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'PROJECT_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installing IBM Watson Machine Learning library\n",
    "\n",
    "NOTE: Documentation could be found [here](http://ibm-wml-api-pyclient.mybluemix.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ibm-watson-machine-learning --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_client = APIClient(WML_CREDENTIALS)\n",
    "print(wml_client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_prepare_data\"></a>\n",
    "## 2. Explore and prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: read from `/data` directory if running locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/imdb_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"sentiment\", data=df)\n",
    "plt.title(\"Sentiment label distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step you will prepare data for training a model. Using the following Text Feature Engineering techniques:\n",
    "\n",
    "1. Tonkenization         \n",
    "2. Removes stop words\n",
    "3. Stemming text (porter)\n",
    "4. Joining words (tokens) into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    \"\"\"Tokenizes the input text into a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing tokens extracted from the input text\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    \"\"\"Remove stop words from a list of tokens\n",
    "\n",
    "    Args:\n",
    "        tokens (list): A list of tokens from which stop words will be removed\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens with stop words removed\n",
    "    \"\"\"\n",
    "    stop = [w for w in tokens if not w in stop_words]\n",
    "    return (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_porter(tokens):\n",
    "    \"\"\"Execute Porter stemming on a list of tokens\n",
    "\n",
    "    Args:\n",
    "        row (list): A list of tokens to apply stemming\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens after stemming\n",
    "    \"\"\"\n",
    "    stemmed_list = [porter_stemmer.stem(word) for word in tokens]\n",
    "    return (stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(tokens):\n",
    "    \"\"\"Join tokens in a single string\n",
    "\n",
    "    Args:\n",
    "        tokens (list of str): List of tokens to be joined\n",
    "\n",
    "    Returns:\n",
    "        str: The text obtained by joining the tokens with spaces\n",
    "    \"\"\"\n",
    "    joined_words = (\" \".join(tokens))\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df, column):\n",
    "    \"\"\"Execute text feature engineering (TFE)\n",
    "    Args:\n",
    "        df (dataframe): row of dataframe\n",
    "        column: column containing text to be processed\n",
    "    Returns:\n",
    "        list: Text post text feature engineering (TFE)\n",
    "    \"\"\"  \n",
    "    df_col = df[column]\n",
    "\n",
    "    input_tokens = df_col.apply(tokenization)\n",
    "    input_tokens = input_tokens.apply(remove_stop_words)\n",
    "    input_tokens = input_tokens.apply(stem_porter)\n",
    "    input_text_cleaned = input_tokens.apply(rejoin_words)\n",
    "\n",
    "    df['cleaned_text'] = input_text_cleaned\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_processing(df, \"review\")\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_test_set\"></a>\n",
    "## 3. Create train and test dataset\n",
    "\n",
    "NOTE: Test dataset (30%) and Training dataset (70%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']\n",
    "Y = df['sentiment']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Machine Learning or Deep Learning models uses numeric values input. The Tf-Idf Text Feature Engineering (TFE) process will be used to transform the texts into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(2,3), sublinear_tf=True)\n",
    "\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "\n",
    "print(Y.value_counts().shape)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y_train_le = le.fit_transform(list(Y_train))\n",
    "Y_test_le = le.transform(list(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_model\"></a>\n",
    "## 4. Train the model\n",
    "\n",
    "Create a Scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classifiers\n",
    "# GradientBoostingClassifier\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "gradient_boost.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_gradient_boost = gradient_boost.predict(X_test_tf)\n",
    "print('Gradient Boosting Classifier DONE!')\n",
    "\n",
    "# SVC -- too much time\n",
    "# svc_model = SVC(gamma='auto', kernel='sigmoid', C=1.8, probability=True)\n",
    "# svc_model.fit(X_train_tf, Y_train_le)\n",
    "# Y_predict_svm = svc_model.predict(X_test_tf)\n",
    "# print('Support Vector Machine(SVM) DONE!')\n",
    "\n",
    "# RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "random_forest.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_random_forest = random_forest.predict(X_test_tf)\n",
    "print('Random Forest Classifier DONE!')\n",
    "\n",
    "# KNeighborsClassifier\n",
    "k_neighbors = KNeighborsClassifier()\n",
    "k_neighbors.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_k_neighbors = k_neighbors.predict(X_test_tf)\n",
    "print('K Nearest Neighbor Classifier DONE!')\n",
    "\n",
    "# LogisticRegression\n",
    "logistic_regression = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
    "logistic_regression.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_logistic_regression = logistic_regression.predict(X_test_tf)\n",
    "print('Logistic Regression DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier:  ', metrics.accuracy_score(Y_test_le, Y_predict_gradient_boost))\n",
    "# print('Support Vector Machine(SVM):   ', metrics.accuracy_score(Y_test_le, Y_predict_svm))\n",
    "print('Random Forest Classifier:      ', metrics.accuracy_score(Y_test_le, Y_predict_random_forest))\n",
    "print('K Nearest Neighbor Classifier: ', metrics.accuracy_score(Y_test_le, Y_predict_k_neighbors))\n",
    "print('Logistic Regression:           ', metrics.accuracy_score(Y_test_le, Y_predict_logistic_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Resume of classification\n",
    "\n",
    "NOTE: Accuracy >= 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_svm)))\n",
    "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_random_forest)))\n",
    "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_logistic_regression)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Logistic Regression\n",
    "\n",
    "NOTE: Best accuracy model confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_logistic_regression)\n",
    "sns.heatmap(logistic_regression_conf_matrix, annot=True,  fmt='');\n",
    "plt.title('Confusion matrix, Logistic Regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Best accuracy model ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test_le, Y_predict_logistic_regression)\n",
    "auc = metrics.roc_auc_score(Y_test_le, Y_predict_logistic_regression)\n",
    "\n",
    "plt.subplots(figsize=(5,5))\n",
    "plt.plot(fpr,tpr, label=\"AUC=\"+str(auc))\n",
    "plt.plot(np.linspace(0, 1, 100),\n",
    "         np.linspace(0, 1, 100),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=18)\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = tfidf.fit_transform(X)\n",
    "Y_train_final = le.fit_transform(list(Y))\n",
    "\n",
    "print(X_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
    "lrc.fit(X_train_final, Y_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save_model\"></a>\n",
    "## 5. Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Save the model to IBM Watson Studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.set.default_project(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofware_spec_uid = wml_client.software_specifications.get_id_by_name(\"runtime-22.1-py3.9\")\n",
    "metadata = {\n",
    "            wml_client.repository.ModelMetaNames.NAME: 'Logistic Regression model to predict IMDB reviews',\n",
    "            wml_client.repository.ModelMetaNames.TYPE: 'scikit-learn_1.0',\n",
    "            wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n",
    "}\n",
    "\n",
    "published_model = wml_client.repository.store_model(model=lrc, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Save the model to IBM Watson Studio space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.set.default_space(SPACE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model = wml_client.repository.store_model(model=lrc, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = wml_client.repository.get_model_id(published_model)\n",
    "model_details = wml_client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wml_client.repository.delete(published_model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy_model\"></a>\n",
    "## 6. Deploy and score\n",
    "\n",
    "NOTE: Deploy and score the model deployed at IBM Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    wml_client.deployments.ConfigurationMetaNames.NAME: \"Deployment of IMDB reviews model\",\n",
    "    wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "created_deployment = wml_client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get deployment UID and show details on the deployment\n",
    "deployment_uid = wml_client.deployments.get_uid(created_deployment)\n",
    "wml_client.deployments.get_details(deployment_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.deployments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wml_client.deployments.delete(deployment_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Score model\n",
    "\n",
    "NOTE: Test the API created from IBM Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scoring end point\n",
    "scoring_endpoint = wml_client.deployments.get_scoring_href(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some test data\n",
    "scoring_payload = {\"input_data\": [\n",
    "    {'values': X_test_tf.toarray()\n",
    "    }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "predictions = wml_client.deployments.score(deployment_uid, scoring_payload)\n",
    "print('prediction',json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_final_model = []\n",
    "for y in predictions['predictions'][0]['values']:\n",
    "    Y_predict_final_model.append(y[0])\n",
    "    \n",
    "print('Final Model WML:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_final_model)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
