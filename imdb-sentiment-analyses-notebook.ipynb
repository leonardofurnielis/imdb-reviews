{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IMDB Sentiment Analyses\n",
    "\n",
    "Neste notebook estamos utilizando os dados do Kaggle (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n",
    "\n",
    "Vamos seguir os seguintes passos:\n",
    "\n",
    "1. Importar o dataset\n",
    "2. Analisar os dados\n",
    "3. Preparar os dados para construir o modelo\n",
    "4. Criar o dataset de teste e treino\n",
    "5. Treinar o modelo utilizando diferentes algoritmos\n",
    "6. Avaliar os modelos\n",
    "7. Seleção do melhor modelo para este dataset\n",
    "8. Realizar o deploy do modelo para o Watson Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_cff28c1bbcb74e9e8ddb271b5109fdc4 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='q-aaHeH2Nc4XQHFBXPmoojJ5mWfTZ-NaKX8Uml0G1rXb',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_cff28c1bbcb74e9e8ddb271b5109fdc4.get_object(Bucket='imdbsentimentanalyses-donotdelete-pr-lxalf6ovy8cv0p',Key='imdb-dataset.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df_data_1 = pd.read_csv(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data_1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando dos dados\n",
    "\n",
    "Agora vamos preparar nossos seguindo os passos:\n",
    "\n",
    "    1. Tonkenization         \n",
    "    2. Remover stopwords\n",
    "    3. Stemming text\n",
    "    4. Juntar novamente em uma única frase\n",
    "    \n",
    "Como estamos trabalhando com uma entrada de texto, realizamos estas etapas para \"normalizar\" nossa base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    source = row[0]\n",
    "    tokens = word_tokenize(source)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(row):\n",
    "    source_tokenization = row[2]\n",
    "    stop = [w for w in source_tokenization if not w in stop_words]\n",
    "    return (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_porter(row):\n",
    "    my_list = row[2]\n",
    "    stemmed_list = [porter_stemmer.stem(word) for word in my_list]\n",
    "    return (stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    my_list = row[2]\n",
    "    joined_words = (\" \".join(my_list))\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    print('Tokenization')\n",
    "    df['text1'] = df.apply(identify_tokens, axis=1)\n",
    "    print('Remove stop words')\n",
    "    df['text1'] = df.apply(remove_stops, axis=1)\n",
    "    print('Stemming')\n",
    "    df['text1'] = df.apply(stem_porter, axis=1)\n",
    "    print('Rejoin words')\n",
    "    df['tidy_text'] = df.apply(rejoin_words, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_processing(df)\n",
    "\n",
    "df['tidy_text'] = df['tidy_text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o dataset de teste/treino\n",
    "\n",
    "Vamos criar o nosso dataset de teste (30%) e treino (70%) de forma balanceado (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tidy_text']\n",
    "Y = df['sentiment']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos de Machine Learning ou Deep Learning esperam como entrada \"X\" um valor numérico. Como estamos trabalhando com texto iremos realizar o processo de TfIdf para transformar o texto em valores numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(2,3), sublinear_tf=True)\n",
    "\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "\n",
    "print(Y.value_counts().shape)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y_train_le = le.fit_transform(list(Y_train))\n",
    "Y_test_le = le.transform(list(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo e treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classifiers\n",
    "# GradientBoostingClassifier\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "gradient_boost.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_gradient_boost = gradient_boost.predict(X_test_tf)\n",
    "print('Gradient Boosting Classifier DONE!')\n",
    "\n",
    "# SVC\n",
    "svc_model = SVC(gamma='auto', kernel='sigmoid', C=1.8, probability=True)\n",
    "svc_model.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_svm = svc_model.predict(X_test_tf)\n",
    "print('Support Vector Machine(SVM) DONE!')\n",
    "\n",
    "# RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "random_forest.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_random_forest = random_forest.predict(X_test_tf)\n",
    "print('Random Forest Classifier DONE!')\n",
    "\n",
    "# KNeighborsClassifier\n",
    "k_neighbors = KNeighborsClassifier()\n",
    "k_neighbors.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_k_neighbors = k_neighbors.predict(X_test_tf)\n",
    "print('K Nearest Neighbor Classifier DONE!')\n",
    "\n",
    "# LogisticRegression\n",
    "logistic_regression = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
    "logistic_regression.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_logistic_regression = logistic_regression.predict(X_test_tf)\n",
    "print('Logistic Regression DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier:  ', metrics.accuracy_score(Y_test_le, Y_predict_gradient_boost))\n",
    "print('Support Vector Machine(SVM):   ', metrics.accuracy_score(Y_test_le, Y_predict_svm))\n",
    "print('Random Forest Classifier:      ', metrics.accuracy_score(Y_test_le, Y_predict_random_forest))\n",
    "print('K Nearest Neighbor Classifier: ', metrics.accuracy_score(Y_test_le, Y_predict_k_neighbors))\n",
    "print('Logistic Regression:           ', metrics.accuracy_score(Y_test_le, Y_predict_logistic_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_svc_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_svm)\n",
    "sns.heatmap(svm_svc_conf_matrix, annot=True,  fmt='');\n",
    "title = 'SVM'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_random_forest)\n",
    "sns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\n",
    "title = 'Random Forest'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_logistic_regression)\n",
    "sns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\n",
    "title = 'Logistic Regression'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_svm)))\n",
    "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_random_forest)))\n",
    "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_logistic_regression)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção do modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = tfidf.fit_transform(X)\n",
    "Y_train_final = le.fit_transform(list(Y))\n",
    "\n",
    "print(X_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
    "lrc.fit(X_train_final, Y_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy para o Watson Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Para nos autenticar no Watson Machine Learning no IBM Cloud, você precisa da api_key e location do seu serviço.\n",
    "\n",
    "Podemos utilizar o [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) ou diretamente pelo portal do IBM Cloud.\n",
    "\n",
    "Usando o IBM Cloud CLI:\n",
    "\n",
    "```\n",
    "ibmcloud login\n",
    "ibmcloud iam api-key-create API_KEY_NAME\n",
    "```\n",
    "\n",
    "NOTE: Você pode obter a URL do serviço indo até [Endpoint URLs section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'YOUR_API_KEY'\n",
    "location = 'YOUR_LOCATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "    \"apikey\": api_key,\n",
    "    \"url\": location\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando a biblioteca do Watson Machine Learning\n",
    "\n",
    "NOTE: Documentação pode ser encontrada [aqui](http://ibm-wml-api-pyclient.mybluemix.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ibm-watson-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "client = APIClient(wml_credentials)\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando nosso espaço de implementação\n",
    "\n",
    "Primeiro, crie um espaço de implementação que será usado para fazer o deploy do nosso modelo. Caso ainda não tenha criado siga os passos abaixo.\n",
    "\n",
    "    Clique em Novo Espaço de Implementação\n",
    "    Crie um novo espaço vazio\n",
    "    Selecione Cloud Object Storage\n",
    "    Selecione Watson Machine Learning e clique em Criar\n",
    "    Copie space_id e cole abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_id = 'YOUR_SPACE_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofware_spec_uid = client.software_specifications.get_id_by_name(\"default_py3.7\")\n",
    "metadata = {\n",
    "            client.repository.ModelMetaNames.NAME: 'Logistic Regression model to predict IMDB reviews',\n",
    "            client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n",
    "            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n",
    "}\n",
    "\n",
    "published_model = client.repository.store_model(\n",
    "    model=lrc,\n",
    "    meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.repository.delete('GUID of stored model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of IMDB reviews\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get deployment UID and show details on the deployment\n",
    "deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "client.deployments.get_details(deployment_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.deployments.delete('GUID of deployed model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando o modelo\n",
    "\n",
    "Agora vamos enviar dados para o web service usando o método score do WML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scoring end point\n",
    "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some test data\n",
    "scoring_payload = {\"input_data\": [\n",
    "    {'values': X_test_tf.toarray()\n",
    "    }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "predictions = client.deployments.score(deployment_uid, scoring_payload)\n",
    "print('prediction',json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_final_model = []\n",
    "for y in predictions['predictions'][0]['values']:\n",
    "    Y_predict_final_model.append(y[0])\n",
    "    \n",
    "print('Final Model WML:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_final_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
