{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "# IMDB Reviews Sentiment Analyses\n",
                "\n",
                "Neste notebook estamos utilizando os dados do Kaggle (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n",
                "\n",
                "Vamos seguir os seguintes passos:\n",
                "\n",
                "1. Importar o Dataset\n",
                "2. Analisar os Dados\n",
                "3. Preparar os Dados para Construir o Modelo\n",
                "4. Criar o Dataset de Teste e Treino\n",
                "5. Treino do Modelo Utilizando Diferentes Algoritmos\n",
                "6. Avaliação dos Modelos\n",
                "7. Seleção do Melhor Modelo\n",
                "8. Deploy do Modelo para o Watson Machine Learning\n",
                "9. Avaliação do Modelo Final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline \n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib as mlp\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import json\n",
                "\n",
                "from nltk.stem import PorterStemmer\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn import preprocessing\n",
                "from sklearn import tree\n",
                "from sklearn import svm\n",
                "from sklearn import ensemble\n",
                "from sklearn import neighbors\n",
                "from sklearn import linear_model\n",
                "from sklearn import metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "nltk.download('stopwords')\n",
                "nltk.download('punkt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1 - Importar o Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The code was removed by IBM Watson Studio for sharing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2 - Analisar dos dados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df_data_1\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3 - Preparar os Dados para Construir o Modelo\n",
                "\n",
                "Agora vamos preparar nossos seguindo os passos:\n",
                "\n",
                "    1. Tonkenization         \n",
                "    2. Remover stopwords\n",
                "    3. Stemming text\n",
                "    4. Juntar novamente em uma única frase\n",
                "    \n",
                "Como estamos trabalhando com uma entrada de texto, realizamos estas etapas para \"normalizar\" nossa base."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stop_words = stopwords.words('english')\n",
                "porter_stemmer = PorterStemmer()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def identify_tokens(row):\n",
                "    source = row[0]\n",
                "    tokens = word_tokenize(source)\n",
                "    token_words = [w for w in tokens if w.isalpha()]\n",
                "    return token_words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_stops(row):\n",
                "    source_tokenization = row[2]\n",
                "    stop = [w for w in source_tokenization if not w in stop_words]\n",
                "    return (stop)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def stem_porter(row):\n",
                "    my_list = row[2]\n",
                "    stemmed_list = [porter_stemmer.stem(word) for word in my_list]\n",
                "    return (stemmed_list)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rejoin_words(row):\n",
                "    my_list = row[2]\n",
                "    joined_words = (\" \".join(my_list))\n",
                "    return joined_words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pre_processing(df):\n",
                "    print('Tokenization')\n",
                "    df['text1'] = df.apply(identify_tokens, axis=1)\n",
                "    print('Remove stop words')\n",
                "    df['text1'] = df.apply(remove_stops, axis=1)\n",
                "    print('Stemming')\n",
                "    df['text1'] = df.apply(stem_porter, axis=1)\n",
                "    print('Rejoin words')\n",
                "    df['tidy_text'] = df.apply(rejoin_words, axis=1)\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pre_processing(df)\n",
                "\n",
                "df['tidy_text'] = df['tidy_text'].str.lower()\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4 - Criar o Dataset de Teste e Treino\n",
                "\n",
                "Vamos criar o nosso dataset de teste (30%) e treino (70%) de forma balanceado (Stratified)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df['tidy_text']\n",
                "Y = df['sentiment']\n",
                "\n",
                "print(X.shape)\n",
                "print(Y.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Os modelos de Machine Learning ou Deep Learning esperam como entrada \"X\" um valor numérico. Como estamos trabalhando com texto iremos realizar o processo de Text Feature Engineering (TFE). Utilizaremos o Tf-Idf para transformar o texto em valores numéricos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(2,3), sublinear_tf=True)\n",
                "\n",
                "X_train_tf = tfidf.fit_transform(X_train)\n",
                "X_test_tf = tfidf.transform(X_test)\n",
                "\n",
                "print(Y.value_counts().shape)\n",
                "print(X_train_tf.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "le = preprocessing.LabelEncoder()\n",
                "\n",
                "Y_train_le = le.fit_transform(list(Y_train))\n",
                "Y_test_le = le.transform(list(Y_test))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5 - Treino do Modelo Utilizando Diferentes Algoritmos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.linear_model import LogisticRegression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# binary classifiers\n",
                "# GradientBoostingClassifier\n",
                "gradient_boost = GradientBoostingClassifier()\n",
                "gradient_boost.fit(X_train_tf, Y_train_le)\n",
                "Y_predict_gradient_boost = gradient_boost.predict(X_test_tf)\n",
                "print('Gradient Boosting Classifier DONE!')\n",
                "\n",
                "# SVC\n",
                "svc_model = SVC(gamma='auto', kernel='sigmoid', C=1.8, probability=True)\n",
                "svc_model.fit(X_train_tf, Y_train_le)\n",
                "Y_predict_svm = svc_model.predict(X_test_tf)\n",
                "print('Support Vector Machine(SVM) DONE!')\n",
                "\n",
                "# RandomForestClassifier\n",
                "random_forest = RandomForestClassifier(n_estimators=10)\n",
                "random_forest.fit(X_train_tf, Y_train_le)\n",
                "Y_predict_random_forest = random_forest.predict(X_test_tf)\n",
                "print('Random Forest Classifier DONE!')\n",
                "\n",
                "# KNeighborsClassifier\n",
                "k_neighbors = KNeighborsClassifier()\n",
                "k_neighbors.fit(X_train_tf, Y_train_le)\n",
                "Y_predict_k_neighbors = k_neighbors.predict(X_test_tf)\n",
                "print('K Nearest Neighbor Classifier DONE!')\n",
                "\n",
                "# LogisticRegression\n",
                "logistic_regression = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
                "logistic_regression.fit(X_train_tf, Y_train_le)\n",
                "Y_predict_logistic_regression = logistic_regression.predict(X_test_tf)\n",
                "print('Logistic Regression DONE!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Gradient Boosting Classifier:  ', metrics.accuracy_score(Y_test_le, Y_predict_gradient_boost))\n",
                "print('Support Vector Machine(SVM):   ', metrics.accuracy_score(Y_test_le, Y_predict_svm))\n",
                "print('Random Forest Classifier:      ', metrics.accuracy_score(Y_test_le, Y_predict_random_forest))\n",
                "print('K Nearest Neighbor Classifier: ', metrics.accuracy_score(Y_test_le, Y_predict_k_neighbors))\n",
                "print('Logistic Regression:           ', metrics.accuracy_score(Y_test_le, Y_predict_logistic_regression))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6 - Avaliação dos Modelos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.1 - Support Vector Machines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "svm_svc_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_svm)\n",
                "sns.heatmap(svm_svc_conf_matrix, annot=True,  fmt='');\n",
                "title = 'SVM'\n",
                "plt.title(title);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.2 - Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_forest_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_random_forest)\n",
                "sns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\n",
                "title = 'Random Forest'\n",
                "plt.title(title);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.3 - Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logistic_regression_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_logistic_regression)\n",
                "sns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\n",
                "title = 'Logistic Regression'\n",
                "plt.title(title);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.4 - Resumo de classificação"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_svm)))\n",
                "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_random_forest)))\n",
                "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_logistic_regression)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 7 - Seleção do Melhor Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_final = tfidf.fit_transform(X)\n",
                "Y_train_final = le.fit_transform(list(Y))\n",
                "\n",
                "print(X_train_final.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lrc = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
                "lrc.fit(X_train_final, Y_train_final)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 8 - Deploy do Modelo para o Watson Machine Learning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "Para nos autenticar no Watson Machine Learning no IBM Cloud, você precisa da api_key e location do seu serviço.\n",
                "\n",
                "Podemos utilizar o [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) ou diretamente pelo portal do IBM Cloud.\n",
                "\n",
                "Usando o IBM Cloud CLI:\n",
                "\n",
                "```\n",
                "ibmcloud login\n",
                "ibmcloud iam api-key-create API_KEY_NAME\n",
                "```\n",
                "\n",
                "NOTE: Você pode obter a URL do serviço indo até [Endpoint URLs section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "api_key = 'YOUR_API_KEY'\n",
                "location = 'YOUR_LOCATION'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wml_credentials = {\n",
                "    \"apikey\": api_key,\n",
                "    \"url\": location\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8.1 - Instalando a biblioteca do Watson Machine Learning\n",
                "\n",
                "NOTE: Documentação pode ser encontrada [aqui](http://ibm-wml-api-pyclient.mybluemix.net/)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -U ibm-watson-machine-learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ibm_watson_machine_learning import APIClient\n",
                "\n",
                "client = APIClient(wml_credentials)\n",
                "print(client.version)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8.2 - Criando nosso espaço de implementação\n",
                "\n",
                "Primeiro, crie um espaço de implementação que será usado para fazer o deploy do nosso modelo. Caso ainda não tenha criado siga os passos abaixo.\n",
                "\n",
                "    1. Clique em Novo Espaço de Implementação\n",
                "    2. Crie um novo espaço vazio\n",
                "    3. Selecione Cloud Object Storage\n",
                "    4. Selecione Watson Machine Learning e clique em Criar\n",
                "    5. Copie space_id e cole abaixo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "space_id = 'YOUR_SPACE_ID'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.spaces.list(limit=10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.set.default_space(space_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sofware_spec_uid = client.software_specifications.get_id_by_name(\"default_py3.8\")\n",
                "metadata = {\n",
                "            client.repository.ModelMetaNames.NAME: 'Logistic Regression model to predict IMDB reviews',\n",
                "            client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n",
                "            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n",
                "}\n",
                "\n",
                "published_model = client.repository.store_model(\n",
                "    model=lrc,\n",
                "    meta_props=metadata)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "published_model_uid = client.repository.get_model_uid(published_model)\n",
                "model_details = client.repository.get_details(published_model_uid)\n",
                "print(json.dumps(model_details, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.repository.list_models()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# client.repository.delete('GUID of stored model')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metadata = {\n",
                "    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of IMDB reviews\",\n",
                "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
                "}\n",
                "\n",
                "created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get deployment UID and show details on the deployment\n",
                "deployment_uid = client.deployments.get_uid(created_deployment)\n",
                "client.deployments.get_details(deployment_uid)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "client.deployments.list()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#client.deployments.delete('GUID of deployed model')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 9 - Avaliação do Modelo Final\n",
                "\n",
                "Agora vamos enviar dados para o web service usando o método score do WML."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get scoring end point\n",
                "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\n",
                "print(scoring_endpoint)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add some test data\n",
                "scoring_payload = {\"input_data\": [\n",
                "    {'values': X_test_tf.toarray()\n",
                "    }]}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# score the model\n",
                "predictions = client.deployments.score(deployment_uid, scoring_payload)\n",
                "print('prediction',json.dumps(predictions, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Y_predict_final_model = []\n",
                "for y in predictions['predictions'][0]['values']:\n",
                "    Y_predict_final_model.append(y[0])\n",
                "    \n",
                "print('Final Model WML:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_final_model)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
