{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# IMDB Reviews Sentiment Analyses\n\nNeste notebook ser\u00e1 utilizado os dados do Kaggle (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n\n1. Importar o Dataset\n2. Analisar os Dados\n3. Preparar os Dados para Construir o Modelo\n4. Criar o Dataset de Teste e Treino\n5. Treino do Modelo Utilizando Diferentes Algoritmos\n6. Avalia\u00e7\u00e3o dos Modelos\n7. Sele\u00e7\u00e3o do Melhor Modelo\n8. Deploy do Modelo para o Watson Machine Learning\n9. Avalia\u00e7\u00e3o do Modelo Final"}, {"metadata": {}, "cell_type": "code", "source": "!pip install nltk", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn import ensemble\nfrom sklearn import neighbors\nfrom sklearn import linear_model\nfrom sklearn import metrics", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import nltk\nnltk.download('stopwords')\nnltk.download('punkt')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 1 - Importar o Dataset"}, {"metadata": {}, "cell_type": "code", "source": "# The code was removed by IBM Watson Studio for sharing.", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2 - Analisar dos dados"}, {"metadata": {}, "cell_type": "code", "source": "df = df_data_1\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 3 - Preparar os Dados para Construir o Modelo\n\nPrepara\u00e7\u00e3o dos dados utilizando as seguintes t\u00e9cnicas de Text Feature Engineering:\n\n    1. Tonkenization         \n    2. Remover stopwords\n    3. Stemming text\n    4. Juntar novamente em uma \u00fanica frase"}, {"metadata": {}, "cell_type": "code", "source": "stop_words = stopwords.words('english')\nporter_stemmer = PorterStemmer()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def identify_tokens(row):\n    source = row[0]\n    tokens = word_tokenize(source)\n    token_words = [w for w in tokens if w.isalpha()]\n    return token_words", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def remove_stops(row):\n    source_tokenization = row[2]\n    stop = [w for w in source_tokenization if not w in stop_words]\n    return (stop)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def stem_porter(row):\n    my_list = row[2]\n    stemmed_list = [porter_stemmer.stem(word) for word in my_list]\n    return (stemmed_list)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def rejoin_words(row):\n    my_list = row[2]\n    joined_words = (\" \".join(my_list))\n    return joined_words", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def pre_processing(df):\n    print('Tokenization')\n    df['text1'] = df.apply(identify_tokens, axis=1)\n    print('Remove stop words')\n    df['text1'] = df.apply(remove_stops, axis=1)\n    print('Stemming')\n    df['text1'] = df.apply(stem_porter, axis=1)\n    print('Rejoin words')\n    df['tidy_text'] = df.apply(rejoin_words, axis=1)\n    print('DONE!')\n    \n    return df", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df = pre_processing(df)\n\ndf['tidy_text'] = df['tidy_text'].str.lower()\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 4 - Criar o Dataset de Teste e Treino\nNOTE: Dataset de teste (30%) e treino (70%) de forma balanceado (Stratified)"}, {"metadata": {}, "cell_type": "code", "source": "X = df['tidy_text']\nY = df['sentiment']\n\nprint(X.shape)\nprint(Y.shape)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "NOTE: Modelos de Machine Learning ou Deep Learning esperam como entrada \"X\" valores num\u00e9ricos. Ser\u00e1 utilizado o processo de Text Feature Engineering (TFE) Tf-Idf para transformar os textos em valores num\u00e9ricos."}, {"metadata": {}, "cell_type": "code", "source": "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(2,3), sublinear_tf=True)\n\nX_train_tf = tfidf.fit_transform(X_train)\nX_test_tf = tfidf.transform(X_test)\n\nprint(Y.value_counts().shape)\nprint(X_train_tf.shape)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "le = preprocessing.LabelEncoder()\n\nY_train_le = le.fit_transform(list(Y_train))\nY_test_le = le.transform(list(Y_test))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 5 - Treino do Modelo Utilizando Diferentes Algoritmos"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Binary classifiers\n# GradientBoostingClassifier\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X_train_tf, Y_train_le)\nY_predict_gradient_boost = gradient_boost.predict(X_test_tf)\nprint('Gradient Boosting Classifier DONE!')\n\n# SVC\nsvc_model = SVC(gamma='auto', kernel='sigmoid', C=1.8, probability=True)\nsvc_model.fit(X_train_tf, Y_train_le)\nY_predict_svm = svc_model.predict(X_test_tf)\nprint('Support Vector Machine(SVM) DONE!')\n\n# RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(X_train_tf, Y_train_le)\nY_predict_random_forest = random_forest.predict(X_test_tf)\nprint('Random Forest Classifier DONE!')\n\n# KNeighborsClassifier\nk_neighbors = KNeighborsClassifier()\nk_neighbors.fit(X_train_tf, Y_train_le)\nY_predict_k_neighbors = k_neighbors.predict(X_test_tf)\nprint('K Nearest Neighbor Classifier DONE!')\n\n# LogisticRegression\nlogistic_regression = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\nlogistic_regression.fit(X_train_tf, Y_train_le)\nY_predict_logistic_regression = logistic_regression.predict(X_test_tf)\nprint('Logistic Regression DONE!')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print('Gradient Boosting Classifier:  ', metrics.accuracy_score(Y_test_le, Y_predict_gradient_boost))\nprint('Support Vector Machine(SVM):   ', metrics.accuracy_score(Y_test_le, Y_predict_svm))\nprint('Random Forest Classifier:      ', metrics.accuracy_score(Y_test_le, Y_predict_random_forest))\nprint('K Nearest Neighbor Classifier: ', metrics.accuracy_score(Y_test_le, Y_predict_k_neighbors))\nprint('Logistic Regression:           ', metrics.accuracy_score(Y_test_le, Y_predict_logistic_regression))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 6 - Avalia\u00e7\u00e3o dos Modelos"}, {"metadata": {}, "cell_type": "markdown", "source": "## 6.1 - Resumo de classifica\u00e7\u00e3o\nNOTE: Accuracy >= 0.70"}, {"metadata": {}, "cell_type": "code", "source": "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_gradient_boost)))\nprint('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_svm)))\nprint('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_random_forest)))\nprint('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_logistic_regression)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 6.3 - Logistic Regression\nNOTE: Matriz de confus\u00e3o do melhor modelo"}, {"metadata": {}, "cell_type": "code", "source": "logistic_regression_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_logistic_regression)\nsns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\ntitle = 'Logistic Regression'\nplt.title(title);", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 7 - Sele\u00e7\u00e3o do Melhor Modelo"}, {"metadata": {}, "cell_type": "code", "source": "X_train_final = tfidf.fit_transform(X)\nY_train_final = le.fit_transform(list(Y))\n\nprint(X_train_final.shape)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "lrc = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\nlrc.fit(X_train_final, Y_train_final)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 8 - Deploy do Modelo para o Watson Machine Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "\n\nPara autenticar no Watson Machine Learning no IBM Cloud, voc\u00ea precisa da api_key e location do seu servi\u00e7o.\n\nPodemos utilizar o [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) ou diretamente pelo portal do IBM Cloud.\n\nUsando o IBM Cloud CLI:\n\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n\nNOTE: Voc\u00ea pode obter a URL do servi\u00e7o indo at\u00e9 [Endpoint URLs section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning)."}, {"metadata": {}, "cell_type": "code", "source": "api_key = 'API_KEY'\nlocation = 'LOCATION'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": location\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 8.1 - Instalando a biblioteca do Watson Machine Learning\n\nNOTE: Documenta\u00e7\u00e3o pode ser encontrada [aqui](http://ibm-wml-api-pyclient.mybluemix.net/)"}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nclient = APIClient(wml_credentials)\nprint(client.version)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 8.2 - Criando nosso espa\u00e7o de implementa\u00e7\u00e3o\n\nCrie um espa\u00e7o de implementa\u00e7\u00e3o pela UI do Watson Studio que ser\u00e1 usado para fazer o deploy do nosso modelo.\n\n    1. Clique em \"Novo Espa\u00e7o de Implementa\u00e7\u00e3o\"\n    2. Selecione Cloud Object Storage\n    3. Selecione Watson Machine Learning e clique em \"Criar\"\n    4. Copie \"space_id\" e cole abaixo"}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'SPACE_ID'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sofware_spec_uid = client.software_specifications.get_id_by_name(\"runtime-22.1-py3.9\")\nmetadata = {\n            client.repository.ModelMetaNames.NAME: 'Logistic Regression model to predict IMDB reviews',\n            client.repository.ModelMetaNames.TYPE: 'scikit-learn_1.0',\n            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n}\n\npublished_model = client.repository.store_model(\n    model=lrc,\n    meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "published_model_uid = client.repository.get_model_id(published_model)\nmodel_details = client.repository.get_details(published_model_uid)\nprint(json.dumps(model_details, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.repository.list_models()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# client.repository.delete('ID of stored model')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of IMDB reviews\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\ncreated_deployment = client.deployments.create(published_model_uid, meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Get deployment UID and show details on the deployment\ndeployment_uid = client.deployments.get_uid(created_deployment)\nclient.deployments.get_details(deployment_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.deployments.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# client.deployments.delete('GUID of deployed model')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 9 - Avalia\u00e7\u00e3o do Modelo Final\n\nNOTE: Testando a nossa API criada com o WML."}, {"metadata": {}, "cell_type": "code", "source": "# get scoring end point\nscoring_endpoint = client.deployments.get_scoring_href(created_deployment)\nprint(scoring_endpoint)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# add some test data\nscoring_payload = {\"input_data\": [\n    {'values': X_test_tf.toarray()\n    }]}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# score the model\npredictions = client.deployments.score(deployment_uid, scoring_payload)\nprint('prediction',json.dumps(predictions, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Y_predict_final_model = []\nfor y in predictions['predictions'][0]['values']:\n    Y_predict_final_model.append(y[0])\n    \nprint('Final Model WML:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_final_model)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.7", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}