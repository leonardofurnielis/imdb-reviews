{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# IMDB reviews sentiment analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Kaggle dataset (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Set up the environment](#setup_environment)\n",
    "1. [Explore and prepare training data](#explore_prepare_data)\n",
    "1. [Create train and test dataset](#train_test_set)\n",
    "1. [Train the model](#train_model)\n",
    "1. [Save the model](#save_model)\n",
    "1. [Deploy and score](#deploy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup_environment\"></a>\n",
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To authenticate to Watson Machine Learning in the IBM Cloud, you need api_key and service location.\n",
    "\n",
    "Using [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) or directly through the IBM Cloud portal.\n",
    "\n",
    "Using IBM Cloud CLI:\n",
    "\n",
    "```\n",
    "ibmcloud login\n",
    "ibmcloud iam api-key-create API_KEY_NAME\n",
    "```\n",
    "\n",
    "NOTE: To get the Service URL [Endpoint URLs section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning).\n",
    "\n",
    "**Action**: Enter your api_key and location in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'API_KEY'\n",
    "LOCATION = 'LOCATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = {\n",
    "    \"apikey\": API_KEY,\n",
    "    \"url\": LOCATION\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Assign space ID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_ID = 'SPACE_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Assign project ID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'PROJECT_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installing IBM Watson Machine Learning library\n",
    "\n",
    "NOTE: Documentation could be found [here](http://ibm-wml-api-pyclient.mybluemix.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ibm-watson-machine-learning --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_client = APIClient(WML_CREDENTIALS)\n",
    "print(wml_client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_prepare_data\"></a>\n",
    "## 2. Explore and prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code was removed by IBM Watson Studio for sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data_1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"sentiment\", data=df)\n",
    "plt.title(\"Sentiment label distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step you will prepare data for training a model. Using the following Text Feature Engineering techniques:\n",
    "\n",
    "1. Tonkenization         \n",
    "2. Removes stop words\n",
    "3. Stemming text (porter)\n",
    "4. Joining words (tokens) into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    \"\"\"Identify tokens in a row\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        list: text splited in tokens\n",
    "    \"\"\"\n",
    "    source = row[0]\n",
    "    tokens = word_tokenize(source)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(row):\n",
    "    \"\"\"Remove stop words from text\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        list: list of tokens without stop words\n",
    "    \"\"\"\n",
    "    source_tokenization = row[2]\n",
    "    stop = [w for w in source_tokenization if not w in stop_words]\n",
    "    return (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_porter(row):\n",
    "    \"\"\"Execute steamming porter\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        list: list of tokens with steamming.\n",
    "    \"\"\"  \n",
    "    my_list = row[2]\n",
    "    stemmed_list = [porter_stemmer.stem(word) for word in my_list]\n",
    "    return (stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    \"\"\"Join tokens in a single string\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        str: text of joined tokens\n",
    "    \"\"\"   \n",
    "    my_list = row[2]\n",
    "    joined_words = (\" \".join(my_list))\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    \"\"\"Execute text feature engineering (TFE)\n",
    "    Args:\n",
    "        df (dataframe): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        list: Text post text feature engineering (TFE)\n",
    "    \"\"\"  \n",
    "    print('Tokenization ...')\n",
    "    df['text1'] = df.apply(identify_tokens, axis=1)\n",
    "    print('Removing stop words ...')\n",
    "    df['text1'] = df.apply(remove_stops, axis=1)\n",
    "    print('Stemming (porter) ...')\n",
    "    df['text1'] = df.apply(stem_porter, axis=1)\n",
    "    print('Joining words ...')\n",
    "    df['clean_text'] = df.apply(rejoin_words, axis=1)\n",
    "    print('DONE!')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_processing(df)\n",
    "\n",
    "df['clean_text'] = df['clean_text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_test_set\"></a>\n",
    "## 3. Create train and test dataset\n",
    "\n",
    "NOTE: Test dataset (30%) and Training dataset (70%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "Y = df['sentiment']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Machine Learning or Deep Learning models uses numeric values input. The Tf-Idf Text Feature Engineering (TFE) process will be used to transform the texts into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(2,3), sublinear_tf=True)\n",
    "\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "\n",
    "print(Y.value_counts().shape)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y_train_le = le.fit_transform(list(Y_train))\n",
    "Y_test_le = le.transform(list(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_model\"></a>\n",
    "## 4. Train the model\n",
    "\n",
    "Create a Scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classifiers\n",
    "# GradientBoostingClassifier\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "gradient_boost.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_gradient_boost = gradient_boost.predict(X_test_tf)\n",
    "print('Gradient Boosting Classifier DONE!')\n",
    "\n",
    "# SVC -- too much time\n",
    "# svc_model = SVC(gamma='auto', kernel='sigmoid', C=1.8, probability=True)\n",
    "# svc_model.fit(X_train_tf, Y_train_le)\n",
    "# Y_predict_svm = svc_model.predict(X_test_tf)\n",
    "# print('Support Vector Machine(SVM) DONE!')\n",
    "\n",
    "# RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "random_forest.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_random_forest = random_forest.predict(X_test_tf)\n",
    "print('Random Forest Classifier DONE!')\n",
    "\n",
    "# KNeighborsClassifier\n",
    "k_neighbors = KNeighborsClassifier()\n",
    "k_neighbors.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_k_neighbors = k_neighbors.predict(X_test_tf)\n",
    "print('K Nearest Neighbor Classifier DONE!')\n",
    "\n",
    "# LogisticRegression\n",
    "logistic_regression = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
    "logistic_regression.fit(X_train_tf, Y_train_le)\n",
    "Y_predict_logistic_regression = logistic_regression.predict(X_test_tf)\n",
    "print('Logistic Regression DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier:  ', metrics.accuracy_score(Y_test_le, Y_predict_gradient_boost))\n",
    "# print('Support Vector Machine(SVM):   ', metrics.accuracy_score(Y_test_le, Y_predict_svm))\n",
    "print('Random Forest Classifier:      ', metrics.accuracy_score(Y_test_le, Y_predict_random_forest))\n",
    "print('K Nearest Neighbor Classifier: ', metrics.accuracy_score(Y_test_le, Y_predict_k_neighbors))\n",
    "print('Logistic Regression:           ', metrics.accuracy_score(Y_test_le, Y_predict_logistic_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Resume of classification\n",
    "\n",
    "NOTE: Accuracy >= 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_svm)))\n",
    "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_random_forest)))\n",
    "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_logistic_regression)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Logistic Regression\n",
    "\n",
    "NOTE: Best accuracy model confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_conf_matrix = metrics.confusion_matrix(Y_test_le, Y_predict_logistic_regression)\n",
    "sns.heatmap(logistic_regression_conf_matrix, annot=True,  fmt='');\n",
    "plt.title('Confusion matrix, Logistic Regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Best accuracy model ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test_le, Y_predict_logistic_regression)\n",
    "auc = metrics.roc_auc_score(Y_test_le, Y_predict_logistic_regression)\n",
    "\n",
    "plt.subplots(figsize=(5,5))\n",
    "plt.plot(fpr,tpr, label=\"AUC=\"+str(auc))\n",
    "plt.plot(np.linspace(0, 1, 100),\n",
    "         np.linspace(0, 1, 100),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=18)\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = tfidf.fit_transform(X)\n",
    "Y_train_final = le.fit_transform(list(Y))\n",
    "\n",
    "print(X_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(solver='lbfgs', penalty='l2', C=1.5)\n",
    "lrc.fit(X_train_final, Y_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save_model\"></a>\n",
    "## 5. Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Save the model to IBM Watson Studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.set.default_project(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofware_spec_uid = wml_client.software_specifications.get_id_by_name(\"runtime-22.1-py3.9\")\n",
    "metadata = {\n",
    "            wml_client.repository.ModelMetaNames.NAME: 'Logistic Regression model to predict IMDB reviews',\n",
    "            wml_client.repository.ModelMetaNames.TYPE: 'scikit-learn_1.0',\n",
    "            wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n",
    "}\n",
    "\n",
    "published_model = wml_client.repository.store_model(model=lrc, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Save the model to IBM Watson Studio space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.set.default_space(SPACE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model = wml_client.repository.store_model(model=lrc, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = wml_client.repository.get_model_id(published_model)\n",
    "model_details = wml_client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wml_client.repository.delete(published_model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy_model\"></a>\n",
    "## 6. Deploy and score\n",
    "\n",
    "NOTE: Deploy and score the model deployed at IBM Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    wml_client.deployments.ConfigurationMetaNames.NAME: \"Deployment of IMDB reviews model\",\n",
    "    wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "created_deployment = wml_client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get deployment UID and show details on the deployment\n",
    "deployment_uid = wml_client.deployments.get_uid(created_deployment)\n",
    "wml_client.deployments.get_details(deployment_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.deployments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wml_client.deployments.delete(deployment_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Score model\n",
    "\n",
    "NOTE: Test the API created from IBM Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scoring end point\n",
    "scoring_endpoint = wml_client.deployments.get_scoring_href(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some test data\n",
    "scoring_payload = {\"input_data\": [\n",
    "    {'values': X_test_tf.toarray()\n",
    "    }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "predictions = wml_client.deployments.score(deployment_uid, scoring_payload)\n",
    "print('prediction',json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_final_model = []\n",
    "for y in predictions['predictions'][0]['values']:\n",
    "    Y_predict_final_model.append(y[0])\n",
    "    \n",
    "print('Final Model WML:\\n {}\\n'.format(metrics.classification_report(Y_test_le, Y_predict_final_model)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
